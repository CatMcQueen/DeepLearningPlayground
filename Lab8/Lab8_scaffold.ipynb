{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from textloader import TextLoader\n",
    "from tensorflow.python.ops.rnn_cell import RNNCell\n",
    "#from mygru import mygru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class mygru( RNNCell ):\n",
    "    def __init__( self, num_units ):\n",
    "        self.num_units = num_units\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.num_units\n",
    "\n",
    "    def __call__( self, inputs, state, scope=None ):\n",
    "        with tf.name_scope(\"evil_gru\") as scope:\n",
    "            init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            xt = inputs\n",
    "            h_minusone = state\n",
    "            Wz = tf.get_variable(\"W_z\", shape = [inputs.get_shape()[1], state.get_shape()[1]], initializer = init)\n",
    "            Wh = tf.get_variable(\"W_h\", shape = [inputs.get_shape()[1], state.get_shape()[1]], initializer = init)\n",
    "            Wr = tf.get_variable(\"W_r\", shape = [inputs.get_shape()[1], state.get_shape()[1]], initializer = init)\n",
    "            Uz = tf.get_variable(\"U_z\", shape = [state.get_shape()[1], state.get_shape()[1]], initializer = init)\n",
    "            Ur = tf.get_variable(\"U_r\", shape = [state.get_shape()[1], state.get_shape()[1]], initializer = init)\n",
    "            Uh = tf.get_variable(\"U_h\", shape = [state.get_shape()[1], state.get_shape()[1]], initializer = init)\n",
    "            bz = tf.get_variable(\"b_z\", shape = [state.get_shape()[1]], initializer = init)\n",
    "            br = tf.get_variable(\"b_r\", shape = [state.get_shape()[1]], initializer = init)\n",
    "            bh = tf.get_variable(\"b_h\", shape = [state.get_shape()[1]], initializer = init)\n",
    "            zt = tf.sigmoid(tf.nn.bias_add((tf.matmul(xt,Wz) + tf.matmul(h_minusone,Uz)) , bz))\n",
    "            rt = tf.sigmoid( tf.nn.bias_add((tf.matmul(xt,Wr) + tf.matmul(h_minusone,Ur)) , br))\n",
    "            h_twid = tf.tanh(tf.nn.bias_add((tf.matmul(xt,Wh) + tf.matmul( tf.multiply(rt, h_minusone), Uh)), bh))\n",
    "            ht = tf.multiply(zt,h_minusone) + tf.multiply((1-zt),  h_twid)\n",
    "        return ht, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "FOUND 186 BATCHES\n",
      "0 0\t4.3010\n",
      "And eo to                                              e    e   \n",
      "1 0\t3.0483\n",
      "And toe the the the the the the the the the the the the the the \n",
      "2 0\t2.8049\n",
      "And toe the the the the the the the the the the the the the the \n",
      "3 0\t2.5956\n",
      "And toe the the the the the the the the the the the the the the \n",
      "4 0\t2.4291\n",
      "And tore the the the the the the rerere the the rerere the the r\n",
      "5 0\t2.2966\n",
      "And the the the the the the reand the reath the reathe word the \n",
      "6 0\t2.1902\n",
      "And the the the the the reore the reore the reath the reath the \n",
      "7 0\t2.1034\n",
      "And the poren the reore the reore the reore the reore the reath \n",
      "8 0\t2.0291\n",
      "And the poren the reore the reore the reore the reath the reath \n",
      "9 0\t1.9630\n",
      "And the pamant the reath the reore the poren the reath the reath\n",
      "10 0\t1.9036\n",
      "And the pamanites the Lamanites the Lamanites the reperen the wi\n",
      "11 0\t1.8506\n",
      "And the pamanites the Lamanites the Lamanites the Lamanites the \n",
      "12 0\t1.8030\n",
      "And the pamanites the Lamanites the Lamanites the Lamanites the \n",
      "13 0\t1.7600\n",
      "And the pamanites the Lamanites the Lamanites the Lamanites the \n",
      "14 0\t1.7208\n",
      "And the pamanites the Lamanites the Lamanites the Lamanites the \n",
      "15 0\t1.6850\n",
      "And the pamanites the Lamanites the Lamanites the Lamanites the \n",
      "16 0\t1.6523\n",
      "And the pamanites the Lamanites the Lamanites the Lamanites the \n",
      "17 0\t1.6224\n",
      "And the came to pass that the land of the Lamanites and the will\n",
      "18 0\t1.5947\n",
      "And the came to pass that they were and the will of the Lamanite\n",
      "19 0\t1.5694\n",
      "And the came to pass that they were and the will of the Lamanite\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "# -------------------------------------------\n",
    "#\n",
    "# Global variables\n",
    "\n",
    "batch_size = 50\n",
    "sample_batch = 1\n",
    "sequence_length = 50\n",
    "sample_seq = [1]\n",
    "\n",
    "data_loader = TextLoader( \".\", batch_size, sequence_length )\n",
    "\n",
    "vocab_size = data_loader.vocab_size  # dimension of one-hot encodings\n",
    "state_dim = 128\n",
    "\n",
    "num_layers = 2\n",
    "c = .0001\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "#\n",
    "\n",
    "# define placeholders for our inputs.  \n",
    "# in_ph is assumed to be [batch_size,sequence_length]\n",
    "# targ_ph is assumed to be [batch_size,sequence_length]\n",
    "\n",
    "in_ph = tf.placeholder( tf.int32, [ batch_size, sequence_length ], name='inputs' )\n",
    "targ_ph = tf.placeholder( tf.int32, [ batch_size, sequence_length ], name='targets' )\n",
    "in_onehot = tf.one_hot( in_ph, vocab_size, name=\"s_input_onehot\" )\n",
    "\n",
    "inputs = tf.split( in_onehot, sequence_length, axis=1 )\n",
    "inputs = [ tf.squeeze(input_, [1]) for input_ in inputs ]\n",
    "targets = tf.split( targ_ph, sequence_length, axis=1 )\n",
    "\n",
    "s_in_ph = tf.placeholder( tf.int32, sample_seq, name='s_inputs' )\n",
    "s_in_onehot = tf.one_hot( s_in_ph, vocab_size, name=\"s_input_onehot\" )\n",
    "s_inputs = tf.split( s_in_onehot, sample_seq, axis=0 )\n",
    "\n",
    "\n",
    "# ------------------\n",
    "# YOUR COMPUTATION GRAPH HERE\n",
    "\n",
    "#comp_graph(batch_size)\n",
    "R = False\n",
    "with tf.name_scope(\"computation_graph\"):\n",
    "    cell0 = mygru( state_dim )\n",
    "    cell1 = mygru( state_dim )\n",
    "    multi_cell = tf.contrib.rnn.MultiRNNCell([cell0, cell1])\n",
    "\n",
    "    initial_state = multi_cell.zero_state(batch_size, tf.float32)\n",
    "    seq_out, final_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, multi_cell)\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    logits = []\n",
    "    for i in xrange(sequence_length):\n",
    "        logits.append(tf.layers.dense(seq_out[i], vocab_size, reuse = R))\n",
    "        R = True\n",
    "    \n",
    "    weights = [1.0 for _ in xrange(sequence_length)]\n",
    "    loss =  tf.contrib.legacy_seq2seq.sequence_loss(logits, targets, weights)\n",
    "    optim = tf.train.AdamOptimizer(learning_rate = c).minimize(loss)\n",
    "\n",
    "\n",
    "# ------------------\n",
    "# YOUR SAMPLER GRAPH HERE\n",
    "\n",
    "#comp_graph(sample_batch, R = True)\n",
    "R = True\n",
    "with tf.name_scope(\"sampler_graph\"):\n",
    "    s_initial_state = multi_cell.zero_state(sample_batch, tf.float32)\n",
    "    s_seq_out, s_final_state = tf.contrib.legacy_seq2seq.rnn_decoder(s_inputs, s_initial_state, multi_cell)\n",
    "    #print s_seq_out\n",
    "    s_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    s_probs  = [tf.layers.dense(s_seq_out[i], vocab_size, reuse = R) for i in xrange(len(s_seq_out))]\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "#\n",
    "\n",
    "def sample( num=200, prime='ab' ):\n",
    "\n",
    "    # prime the pump\n",
    "\n",
    "    # generate an initial state. this will be a list of states, one for\n",
    "    # each layer in the multicell.\n",
    "    s_state = sess.run( s_initial_state )\n",
    "\n",
    "    # for each character, feed it into the sampler graph and\n",
    "    # update the state.\n",
    "    for char in prime[:-1]:\n",
    "        x = np.ravel( data_loader.vocab[char] ).astype('int32')\n",
    "        feed = { s_in_ph:x }\n",
    "        for i, s in enumerate( s_initial_state ):\n",
    "            feed[s] = s_state[i]\n",
    "        s_state = sess.run( s_final_state, feed_dict=feed )\n",
    "\n",
    "    # now we have a primed state vector; we need to start sampling.\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.ravel( data_loader.vocab[char] ).astype('int32')\n",
    "\n",
    "        # plug the most recent character in...\n",
    "        feed = { s_in_ph: x }\n",
    "        for i, s in enumerate( s_initial_state ):\n",
    "            feed[s] = s_state[i]\n",
    "        ops = [s_probs]\n",
    "        ops.extend( list(s_final_state) )\n",
    "\n",
    "        #print x\n",
    "        retval = sess.run( ops, feed_dict=feed )\n",
    "\n",
    "        s_probsv = retval[0]\n",
    "        s_state = retval[1:]\n",
    "                # ...and get a vector of probabilities out!\n",
    "\n",
    "        # now sample (or pick the argmax)\n",
    "        sample = np.argmax( s_probsv[0] )\n",
    "        #sample = np.random.choice( vocab_size, p=s_probsv[0] )\n",
    "\n",
    "        pred = data_loader.chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "\n",
    "    return ret\n",
    "\n",
    "#\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "#\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run( tf.global_variables_initializer() )\n",
    "summary_writer = tf.summary.FileWriter( \"./tf_logs\", graph=sess.graph )\n",
    "\n",
    "lts = []\n",
    "\n",
    "print \"FOUND %d BATCHES\" % data_loader.num_batches\n",
    "\n",
    "for j in range(20):\n",
    "\n",
    "    state = sess.run( initial_state )\n",
    "    data_loader.reset_batch_pointer()\n",
    "\n",
    "    for i in range( data_loader.num_batches ):\n",
    "\n",
    "        x,y = data_loader.next_batch()\n",
    "\n",
    "        # we have to feed in the individual states of the MultiRNN cell\n",
    "        feed = { in_ph: x, targ_ph: y }\n",
    "        for k, s in enumerate( initial_state ):\n",
    "            feed[s] = state[k]\n",
    "\n",
    "        ops = [optim,loss]\n",
    "        ops.extend( list(final_state) )\n",
    "                # retval will have at least 3 entries:\n",
    "        # 0 is None (triggered by the optim op)\n",
    "        # 1 is the loss\n",
    "        # 2+ are the new final states of the MultiRNN cell\n",
    "        retval = sess.run( ops, feed_dict=feed )\n",
    "\n",
    "        lt = retval[1]\n",
    "        state = retval[2:]\n",
    "\n",
    "        if i%1000==0:\n",
    "            print \"%d %d\\t%.4f\" % ( j, i, lt )\n",
    "            lts.append( lt )\n",
    "\n",
    "    print sample( num=60, prime=\"And \" )\n",
    "#    print sample( num=60, prime=\"ababab\" )\n",
    "#    print sample( num=60, prime=\"foo ba\" )\n",
    "#    print sample( num=60, prime=\"abcdab\" )\n",
    "\n",
    "\n",
    "with open(\"alma_gru.txt\", \"w\") as f:\n",
    "    for i in xrange(30):\n",
    "        f.write(sample( num=60, prime=\"And \" ) + \"\\n\")\n",
    "        \n",
    "summary_writer.close()\n",
    "\n",
    "#\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "# ==================================================================\n",
    "#\n",
    "\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.plot( lts )\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the drowned the said them to the goblin last of the sword of\n",
      "And the drogon the goblins shoot of stole the sho sew ollinge to\n",
      "And the drogon the goblin had note as they were had not seemed t\n",
      "And the dround his fanter the stool of filling the dragon hear a\n",
      "And the deepent\n",
      "and the goblin had not to stole the beard and sh\n",
      "And the roor sho the wizards through the rook into the cound of\n",
      "And the rour prase the sward ond the hand of the stord though no\n",
      "And the rour whes was are allown as the goblin was she triggtt a\n",
      "And the rour her was sure it a sime his long, and which ware a s\n",
      "And the roor in a more that they were was the wand had becond th\n",
      "And the was the goblin said Harry shout them, which ward the swo\n",
      "And the was the goblin was she still be the singed the serpon al\n",
      "And the roor in a more that the goblin was the entrout and the w\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Harry Potter Deathly Hallows Ch 26\n",
    "\n",
    "with open(\"gru_hp.txt\", 'r') as f:\n",
    "    file_lines = f.readlines()\n",
    "for k in file_lines:\n",
    "    print(k[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And and his brethren, who was a praceing against the Lamanites,\n",
      "And and his brethren, who was a preaching of the Lord doth comma\n",
      "And and his brethren, who was a preaching of the Lord did not su\n",
      "And and his brethren, the Lamanites did also be a great strength\n",
      "And and his brethren and had received and also the same was nor\n",
      "And an end and tempted upon them into the land of Zarahemla, and\n",
      "And at the time of the land of Zarahemla, and he should be saved\n",
      "And at the time of the would have been spoken by the Lamanites,\n",
      "And at the last day to the land of Zarahemla, and he should be s\n",
      "And at the last day that he would they see that the Lord had sai\n",
      "And at the last day that he would that ye should return them of\n",
      "And at the last day the people of Nephi, that the Lamanites were\n",
      "And at the seashore, and the Lamanites were strength and began t\n",
      "And at the seashore, and the Lamanites were strong and his breth\n",
      "And at the seashore, and they were all the people of Nephi, and \n",
      "And at the seashore, and the Lamanites were all the city of Neph\n",
      "And all the Lamanites were all the city of Nephi, and the church\n",
      "And at the seashore, and the Lord had said unto him: I did not b\n",
      "And all the Lamanites were all the land of Zarahemla, and the Lo\n",
      "And all the Lamanites were and the city of Ammonihah, and the Lo\n",
      "And all the city of Ammonihah was a sent a sent a command the ci\n",
      "And all the spirit of the Lamanites were and the chief captain o\n",
      "And the seed of the Lamanites were and the commandments of the L\n",
      "And the will of the Lamanites were and the people of the Lamanit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Book of Alma text file\n",
    "with open(\"gru_alma.txt\", 'r') as f:\n",
    "    file_lines = f.readlines()\n",
    "for k in file_lines:\n",
    "    print(k[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the trough and see the drow of the sword and the goblins and\n",
      "He made to dispered and the ground her and Hermione had the sti\n",
      "Then goblins and spent and seamen the sword and the gragon into t\n",
      "The goblin a shill seamed the goblin and raver and the was shous\n",
      "And the trough and see the drow of the sword and the goblins and\n",
      "He made to dispered and the ground her and Hermione had the sti\n",
      "Then goblins and spent and seamen the sword and the gragon into t\n",
      "The goblin a shill seamed the goblin and raver and the was shous\n",
      "And the trough and seeped the stood there with a thing with a th\n",
      "He made to and with the sword and the goblin and the sward a sh\n",
      "Then goblins and the still the ston that the goblin and the sward\n",
      "The goblin a shill wet of the sword the could her was she stood \n",
      "And the trough and seeped the stood there with a thing with a th\n",
      "He made to and with the sword and the goblin and the sward a sh\n",
      "Then goblins and the still the ston that the goblin and the sward\n",
      "The goblin a shill wet of the sword the could her was she stood \n",
      "And the trough a crastered and the goblin a she the sword the co\n",
      "He passed the singer still ser on the was the stold the stor an\n",
      "Then goblins still the stood the stone and the stold the stor and\n",
      "The goblin a shill we conder the sward the could he had not to t\n",
      "And the trough a crastered and the goblin a she the sword the co\n",
      "He passed the singer still ser on the was the stold the stor an\n",
      "Then goblins still the stood the stone and the stold the stor and\n",
      "The goblin a shill we conder the sward the could he had not to t\n",
      "And the trough and see the enormoon the stole he had not looking\n",
      "He made to dispered and the ground her and Hermione had the sti\n"
     ]
    }
   ],
   "source": [
    "#### Second HP text with more first words\n",
    "with open(\"new_hp.txt\", 'r') as f:\n",
    "    file_lines = f.readlines()\n",
    "for k in file_lines:\n",
    "    print(k[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
