{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle( file ):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv( x, filter_size=3, stride=2, num_filters=64, is_output=False, name=\"conv\" ):\n",
    "    shape = x.get_shape().as_list()\n",
    "    with tf.name_scope(name) as scope:\n",
    "        W = tf.get_variable(\"conv_{}_W\".format(name), [filter_size, filter_size, x.shape[-1], num_filters], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        B = tf.get_variable(\"conv_{}_B\".format(name), [num_filters], initializer=tf.ones_initializer())\n",
    "        conv = tf.nn.conv2d(x, W, [1, stride, stride, 1] , \"SAME\")\n",
    "        res = tf.nn.bias_add(conv, B)\n",
    "        #print \"conv \", conv.shape, \"res \", res.shape\n",
    "        if not is_output:\n",
    "            res = tf.nn.relu(res)\n",
    "        return res\n",
    "    \n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Convolve x with W by calling the tf.nn.conv2d function\n",
    "      Add the bias\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    ''' \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './cifar-10-batches-py/data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-18ef95e2d7dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'./cifar-10-batches-py/data_batch_1'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.00001\u001b[0m \u001b[0;31m#the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ebe1e93508e5>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './cifar-10-batches-py/data_batch_1'"
     ]
    }
   ],
   "source": [
    "data = unpickle( './cifar-10-batches-py/data_batch_1' )\n",
    "c = .00001 #the learning rate\n",
    "features = data['data']\n",
    "features = (features-np.mean(features,axis=0))/np.std(features,axis=0)\n",
    "labels = data['labels']\n",
    "labels = np.atleast_2d( labels ).T\n",
    "\n",
    "resized_features = []\n",
    "for x in xrange(features.shape[0]):\n",
    "    resized_features.append(np.reshape(features[x], [1,32,32,3]))\n",
    "\n",
    "resized_features = np.array(resized_features)\n",
    "\n",
    "#model_selection.train_test_split\n",
    "split = int(features.shape[0]*.8)\n",
    "train_features = resized_features[:split]\n",
    "train_labels = labels[:split]\n",
    "test_features = resized_features[split:]\n",
    "test_labels = labels[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def fc( x, out_size=50, name=\"fc\" , is_output=False, bias_val=0.0):\n",
    "    \n",
    "    shape = x.get_shape().as_list()\n",
    "    \n",
    "    with tf.name_scope(name) as scope:\n",
    "        #W = tf.get_variable(\"fc_{}_W\".format(name), [in_size, out_size],initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        W = tf.get_variable(\"fc_{}_W\".format(name), [shape[1], out_size], tf.float32, initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        B = tf.get_variable(\"fc_{}_B\".format(name), [out_size], initializer=tf.constant_initializer(bias_val))\n",
    "        res = tf.matmul(x, W) + B\n",
    "        \n",
    "        if not is_output:\n",
    "            res = tf.nn.relu(res)\n",
    "        return res\n",
    "    \n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Multiply x by W and add b\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "\n",
    "    input_data = tf.placeholder( tf.float32, [1,32,32,3] )\n",
    "    y_real = tf.placeholder(tf.int64, [1])\n",
    "\n",
    "    h0 = conv( input_data, name=\"h0\" )     # h0 = [1,32,32,64]\n",
    "    h1 = conv( h0, name=\"h1\" )             # h1 = [1,16,16,64]\n",
    "    h2 = conv( h1, name=\"h2\", stride = 2 ) # h2 = [1,4,4,64] \n",
    "    reshaping = h2.shape[0]*h2.shape[1]*h2.shape[2]*h2.shape[3] #get reshape val (1024)\n",
    "    h2 = tf.reshape(h2,[1, int(reshaping)])           # h2 = [1024,1]\n",
    "    h3 = fc( h2, name=\"h3\" )               # h3 = [50,1]\n",
    "    h4 = fc( h3, out_size = 10, name=\"h4\", is_output = True) # h4 = [10,1]\n",
    "    logit = h4\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_real , logits = logit, name=\"cross_entropy\")\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=c).minimize(cross_entropy)\n",
    "\n",
    "    prediction = tf.argmax(logit, axis=1) #y_real - tf.argmax()\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    writer = tf.summary.FileWriter(\"./tf_lab5\", sess.graph)\n",
    "    sess.run( init )\n",
    "\n",
    "    epochs = 15\n",
    "    tr_accuracy = []\n",
    "    xe_tr = []\n",
    "    te_accuracy = []\n",
    "    xe_te = []\n",
    "\n",
    "    for ep in tqdm(xrange(epochs)): \n",
    "        tr_intermed =  []\n",
    "        te_intermed = []\n",
    "        for im in xrange(len(train_labels)):\n",
    "            _, xe, pred = sess.run([train_step, cross_entropy, prediction], feed_dict = {input_data: train_features[im], y_real: train_labels[im]})\n",
    "            xe_tr.append(xe)\n",
    "            if (pred[0]== 0):\n",
    "                tr_intermed.append(1)\n",
    "        tr_accuracy.append(float(len(tr_intermed))/len(train_labels))\n",
    "\n",
    "        for im in xrange(len(test_labels)):\n",
    "            xe, pred = sess.run([cross_entropy, prediction], feed_dict = {input_data: test_features[im], y_real: test_labels[im]})\n",
    "            xe_te.append(xe)\n",
    "            if (pred[0]== 0):\n",
    "                te_intermed.append(1)\n",
    "        te_accuracy.append(float(len(te_intermed))/len(test_labels))\n",
    "\n",
    "    sess.close()\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "\n",
    "plt.plot( tr_accuracy, label='Training')\n",
    "plt.plot( te_accuracy, label='Test')\n",
    "plt.title(\"Accuracy of Test vs Training\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Percent Accuracy\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pltt\n",
    "\n",
    "tboard = pltt.imread('./tboardlab5.png')\n",
    "pltt.figure(figsize=[15,15])\n",
    "pltt.axis('off')\n",
    "pltt.imshow(tboard)\n",
    "pltt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "def progressive_predict(final_layer, labels_ph, name=\"progressive_predict\"):\n",
    "    with tf.variable_scope(name):\n",
    "        logits = final_layer\n",
    "\n",
    "        predictions = tf.cast(tf.argmax(logits, axis=1), tf.int32, name=\"predictions\")\n",
    "        accuracy, pred_update_op = tf.metrics.accuracy(\n",
    "            labels=labels_ph,\n",
    "            predictions= predictions,\n",
    "            name=\"accuracy\")\n",
    "\n",
    "        return accuracy, pred_update_op\n",
    "\n",
    "\n",
    "def measure_test(cross_entropy, accuracy, name=\"metrics_summary\"):\n",
    "    with tf.variable_scope(name):\n",
    "        # create a summary for our cost and accuracy\n",
    "        tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_op\n",
    "    \n",
    "def measure_train(cross_entropy, accuracy, name=\"metrics_summary\"):\n",
    "    with tf.variable_scope(name):\n",
    "        # create a summary for our cost and accuracy\n",
    "        tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_op\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input_data = tf.placeholder( tf.float32, [1,32,32,3] )\n",
    "    y_real = tf.placeholder(tf.int64, [1])\n",
    "\n",
    "    h0 = conv( input_data, name=\"h0\" )     # h0 = [1,32,32,64]\n",
    "    h1 = conv( h0, name=\"h1\" )             # h1 = [1,16,16,64]\n",
    "    h2 = conv( h1, name=\"h2\", stride = 2 ) # h2 = [1,4,4,64] \n",
    "    reshaping = h2.shape[0]*h2.shape[1]*h2.shape[2]*h2.shape[3] #get reshape val (1024)\n",
    "    h2 = tf.reshape(h2,[1, int(reshaping)])           # h2 = [1024,1]\n",
    "    h3 = fc( h2, name=\"h3\" )               # h3 = [50,1]\n",
    "    h4 = fc( h3, out_size = 10, name=\"h4\", is_output = True) # h4 = [10,1]\n",
    "    logit = h4\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_real , logits = logit, name=\"cross_entropy\")\n",
    "    train = tf.train.AdamOptimizer(learning_rate=c).minimize(cross_entropy)\n",
    "\n",
    "    accuracy, pred_update_op = progressive_predict(logit, y_real)\n",
    "    get_test_metrics = measure_test(cross_entropy, accuracy, name=\"testing_set_metrics\")\n",
    "    get_train_metrics = measure_train(cross_entropy, accuracy, name=\"training_set_metrics\")\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        j = 0\n",
    "        Epoch = 15\n",
    "        writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "\n",
    "        for epoch in tqdm(range(Epoch)):\n",
    "            for i in tqdm(range(len(labels))):\n",
    "                _, _, train_summary = sess.run([train, pred_update_op, get_train_metrics], feed_dict = {input_data: resized_features[i,:], y_real: labels[i,:]})\n",
    "\n",
    "                writer.add_summary(train_summary, epoch * i)\n",
    "\n",
    "                if i % 4 == 0:\n",
    "                    _, test_summary = sess.run([pred_update_op, get_test_metrics], feed_dict = {input_data: resized_features[j,:], y_real: labels[j,:]})\n",
    "\n",
    "                    writer.add_summary(test_summary, epoch * j)\n",
    "                    j += 1\n",
    "\n",
    "        writer.close()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
